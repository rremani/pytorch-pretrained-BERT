{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "import pytorch_pretrained_bert as _bert\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from dougu import flatten, lines\n",
    "def flatten(list_of_lists):\n",
    "    for list in list_of_lists:\n",
    "        for item in list:\n",
    "            yield item\n",
    "\n",
    "_device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "class Bert():\n",
    "\n",
    "    MASK = \"[MASK]\"\n",
    "    CLS = \"[CLS]\"\n",
    "    SEP = \"[SEP]\"\n",
    "\n",
    "#     supported_langs = set(lines(\n",
    "#         Path(__file__).parent / \"data\" / \"bert_langs.wiki\"))\n",
    "\n",
    "    def __init__(self, model, model_name, device=None, half_precision=False):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.device = device or _device\n",
    "        do_lower_case = \"uncased\" in model_name\n",
    "        self.tokenizer = _bert.BertTokenizer.from_pretrained(\n",
    "            self.model_name, do_lower_case=do_lower_case)\n",
    "        maybe_model_wrapper = model.from_pretrained(model_name).to(\n",
    "            device=self.device)\n",
    "        try:\n",
    "            self.model = maybe_model_wrapper.bert\n",
    "        except AttributeError:\n",
    "            self.model = maybe_model_wrapper\n",
    "        if half_precision:\n",
    "            self.model.half()\n",
    "        self.max_len = \\\n",
    "            self.model.embeddings.position_embeddings.weight.size(0)\n",
    "#         self.max_len = 1000\n",
    "        self.dim = self.model.embeddings.position_embeddings.weight.size(1)\n",
    "\n",
    "    def tokenize(self, text, masked_idxs=None):\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        if masked_idxs is not None:\n",
    "            for idx in masked_idxs:\n",
    "                tokenized_text[idx] = self.MASK\n",
    "        # prepend [CLS] and append [SEP]\n",
    "        # see https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_classifier.py#L195  # NOQA\n",
    "        tokenized = [self.CLS] + tokenized_text + [self.SEP]\n",
    "        return tokenized\n",
    "\n",
    "    def tokenize_to_ids(self, text, masked_idxs=None, pad=True):\n",
    "        tokens = self.tokenize(text, masked_idxs)\n",
    "        return self.convert_tokens_to_ids(tokens, pad=pad)\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens, pad=True):\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        ids = torch.tensor([token_ids]).to(device=self.device)\n",
    "        assert ids.size(1) < self.max_len\n",
    "        if pad:\n",
    "            padded_ids = torch.zeros(1, self.max_len).to(ids)\n",
    "            padded_ids[0, :ids.size(1)] = ids\n",
    "            mask = torch.zeros(1, self.max_len).to(ids)\n",
    "            mask[0, :ids.size(1)] = 1\n",
    "            return padded_ids, mask\n",
    "        else:\n",
    "            return ids\n",
    "\n",
    "    def subword_tokenize(self, tokens):\n",
    "        \"\"\"Segment each token into subwords while keeping track of\n",
    "        token boundaries.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens: A sequence of strings, representing input tokens.\n",
    "        Returns\n",
    "        -------\n",
    "        A tuple consisting of:\n",
    "            - A list of subwords, flanked by the special symbols required\n",
    "                by Bert (CLS and SEP).\n",
    "            - An array of indices into the list of subwords, indicating\n",
    "                that the corresponding subword is the start of a new\n",
    "                token. For example, [1, 3, 4, 7] means that the subwords\n",
    "                1, 3, 4, 7 are token starts, while all other subwords\n",
    "                (0, 2, 5, 6, 8...) are in or at the end of tokens.\n",
    "                This list allows selecting Bert hidden states that\n",
    "                represent tokens, which is necessary in sequence\n",
    "                labeling.\n",
    "        \"\"\"\n",
    "        subwords = list(map(self.tokenizer.tokenize, tokens))\n",
    "        subword_lengths = list(map(len, subwords))\n",
    "        subwords = [self.CLS] + list(flatten(subwords)) + [self.SEP]\n",
    "        token_start_idxs = 1 + np.cumsum([0] + subword_lengths[:-1])\n",
    "        return subwords, token_start_idxs\n",
    "\n",
    "    def subword_tokenize_to_ids(self, tokens):\n",
    "        \"\"\"Segment each token into subwords while keeping track of\n",
    "        token boundaries and convert subwords into IDs.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens: A sequence of strings, representing input tokens.\n",
    "        Returns\n",
    "        -------\n",
    "        A tuple consisting of:\n",
    "            - A list of subword IDs, including IDs of the special\n",
    "                symbols (CLS and SEP) required by Bert.\n",
    "            - A mask indicating padding tokens.\n",
    "            - An array of indices into the list of subwords. See\n",
    "                doc of subword_tokenize.\n",
    "        \"\"\"\n",
    "#         import pdb;pdb.set_trace()\n",
    "        subwords, token_start_idxs = self.subword_tokenize(tokens)\n",
    "        subword_ids, mask = self.convert_tokens_to_ids(subwords)\n",
    "        token_starts = torch.zeros(1, self.max_len).to(subword_ids)\n",
    "        token_starts[0, token_start_idxs] = 1\n",
    "        return subword_ids, mask, token_starts\n",
    "\n",
    "    def segment_ids(self, segment1_len, segment2_len):\n",
    "        ids = [0] * segment1_len + [1] * segment2_len\n",
    "        return torch.tensor([ids]).to(device=self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def Model(model_name, **kwargs):\n",
    "        return Bert(_bert.BertModel, model_name, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def ForMaskedLM(model_name, **kwargs):\n",
    "        return Bert(_bert.BertForMaskedLM, model_name, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def ForSequenceClassification(model_name, **kwargs):\n",
    "        return Bert(\n",
    "            _bert.BertForSequenceClassification, model_name, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def ForNextSentencePrediction(model_name, **kwargs):\n",
    "        return Bert(_bert.BertForNextSentencePrediction, model_name, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def ForPreTraining(model_name, **kwargs):\n",
    "        return Bert(_bert.BertForPreTraining, model_name, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def ForQuestionAnswering(model_name, **kwargs):\n",
    "        return Bert(_bert.BertForQuestionAnswering, model_name, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"TAX INVOICE GSTIN 03AUVPS8413J121 ORIGINAL FOR BUYER Ph:\\n 098144-60916 R.S.Printers PLOT NO 5-A UDHYOG VIHAR ,OPP FCI GODN BHATTIAN ROAD VILL BHADUR LUDHIANA\\n rsprinter.Idh@gmail.com INVOICE No. B20 Date 01.05.2018 Name Details of Buyer (Billed To)\\n Name 'EXCEL TRADING CO. ANANDPURI COLONY NOORWALA ROAD\\n LUDHIANA LUDHIANA\\n GSTIN O3AUWPS7596M1ZC\\n State Punjab State Code 03 LUDHIANA\\n GSTIN O3AUWPS7596M1ZC\\n State Punjab State Code 03 TRANSPORT GR.No. WEHICLE No. PVT MARK. DATE ..\\n NO OF CASES.\\n Taxable LIGST CGST\\n Value Amt. Amt. Item Name Qty Rate HSN/\\n SAC Amount SGST\\n Amt. 0% 3.00 6.0 900.00 6.0 TRANSFER STICKER H.D\\n TRANSFER STICKER SIZE LABEL TRANSFER STICKER SIZE LABEL 49089000\\n 49089000\\n 49089000 5000\\n 2000\\n 3530 1.50 15000.00\\n 3000.00\\n 3530.00 15000.00\\n 3000.00\\n 3530.00 6.0 180.00 6.0 900.00\\n 180.00\\n 212.00 1.00 6.0 212.00 6.0 Totals 21530.00 21530.00 1292.00 1292.00\\n 21530.00 TAXABLE VALUE IGST :\\n CGST SGST :\\n Round off 1292.00\\n 1292.00 Bank Details Total Value (in Figures) 24114.00\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_sentences = []\n",
    "# for tokens in sentences:\n",
    "bert=Bert.Model(\"bert-base-cased\")\n",
    "tokens=text.split(' ')\n",
    "features = {}\n",
    "features[\"bert_ids\"], features[\"bert_mask\"], features[\"bert_token_starts\"] = bert.subword_tokenize_to_ids(tokens)\n",
    "featurized_sentences.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "true=torch.Tensor([int(x) for x in list('0000000003000000000000000020400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000100000000000000300000000000000000000000000000000000000')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(featurized_sentences_batch):\n",
    "    bert_batch = [torch.cat([features[key] for features in featurized_sentences], dim=0) for key in (\"bert_ids\", \"bert_mask\", \"bert_token_starts\")]\n",
    "    return bert_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTagger(torch.nn.Module):\n",
    "    \n",
    "        def __init__(self,bert):\n",
    "                self.bert = bert\n",
    "\n",
    "                bert_dim = 786 # (or get the dim from BertEmbeddings)\n",
    "                n_labels = 5  # need to set this for your task\n",
    "                self.out = torch.nn.Linear(bert_dim, n_labels)\n",
    "    \n",
    "        def forward(self, bert_batch, true_labels):\n",
    "                bert_ids, bert_mask, bert_token_starts = bert_batch\n",
    "                # truncate to longest sequence length in batch (usually much smaller than 512) to save GPU RAM\n",
    "                max_length = (bert_mask != 0).max(0)[0].nonzero()[-1].item()\n",
    "                if max_length < bert_ids.shape[1]:\n",
    "                        bert_ids = bert_ids[:, :max_length]\n",
    "                        bert_mask = bert_mask[:, :max_length]\n",
    "\n",
    "                segment_ids = torch.zeros_like(bert_mask)  # dummy segment IDs, since we only have one sentence\n",
    "                bert_last_layer = self.bert(bert_ids, segment_ids)[0][-1]\n",
    "                # select the states representing each token start, for each instance in the batch\n",
    "                bert_token_reprs = [\n",
    "                         layer[starts.nonzero().squeeze(1)]\n",
    "                         for layer, starts in zip(bert_last_layer, bert_token_starts)]\n",
    "                # need to pad because sentence length varies\n",
    "                padded_bert_token_reprs = pad_sequence(\n",
    "                         bert_token_reprs, batch_first=True, padding_value=-1)\n",
    "                # output/classification layer: input bert states and get log probabilities for cross entropy loss\n",
    "                pred_logits = self.log_softmax(self.out(self.dropout(padded_bert_token_reprs)))\n",
    "                mask = true_labels != -1  # I did set label = -1 for all padding tokens somewhere else\n",
    "                loss = cross_entropy(pred_logits, true_labels)\n",
    "                # average/reduce the loss according to the actual number of of predictions (i.e. one prediction per token).\n",
    "                loss /= mask.float().sum()\n",
    "                return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "bert = BertModel.from_pretrained('bert-base-cased').to(device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_batch = collate_fn(featurized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ids, bert_mask, bert_token_starts = bert_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = (bert_mask != 0).max(0)[0].nonzero()[-1].item()\n",
    "if max_length < bert_ids.shape[1]:\n",
    "        bert_ids = bert_ids[:, :max_length]\n",
    "        bert_mask = bert_mask[:, :max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ids = torch.zeros_like(bert_mask)  # dummy segment IDs, since we on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_last_layer = bert(bert_ids, segment_ids)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pad_sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-f3925ec6cc45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m          for layer, starts in zip(bert_last_layer, bert_token_starts)]\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# need to pad because sentence length varies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m padded_bert_token_reprs = pad_sequence(\n\u001b[0m\u001b[1;32m      6\u001b[0m          bert_token_reprs, batch_first=True, padding_value=-1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pad_sequence' is not defined"
     ]
    }
   ],
   "source": [
    "bert_token_reprs = [\n",
    "         layer[starts.nonzero().squeeze(1)]\n",
    "         for layer, starts in zip(bert_last_layer, bert_token_starts)]\n",
    "# need to pad because sentence length varies\n",
    "padded_bert_token_reprs = pad_sequence(\n",
    "         bert_token_reprs, batch_first=True, padding_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
